{
  "default": {
    "defaultTemperature": 1,
    "maxTemperature": 2,
    "minMaxTokens": 4,
    "defaultMaxTokens": 8192,
    "defaultThinkingTokens": 4096,
    "defaultTopP": 1,
    "minTopP": 0,
    "maxTopP": 1,
    "defaultTopK": 0,
    "minTopK": 0,
    "maxTopK": 500,
    "defaultFrequencyPenalty": 0,
    "defaultPresencePenalty": 0,
    "maxFrequencyPenalty": 2,
    "maxPresencePenalty": 2,
    "maxStopSequences": 4,
    "hint": {
      "temperature": "Adjust the randomness: Lower values mean more predictable and repetitive completions. Use lower values for tasks with a definite 'correct' answer, and higher values for responses that are more creative and random.",
      "maxTokens": "The maximum number of tokens to generate as output. The exact limit will be based on the model's total capacity and varies by model. (One token is roughly 4 characters of English text)",
      "stopSequences": "The sequences where the API will stop generating further tokens. Up to 4 sequences.",
      "topP": "Top P (nucleus) adjusts token choices based on probabilities, enhancing diversity for natural, fluent text generation.",
      "topK": "Limits the number of choices for the next predicted word or token, which helps to speed up the generation process and can improve the quality of the generated text",
      "frequencyPenalty": "How much to penalize new tokens based on their existing frequency in the text so far. Decreases the model's likelihood to repeat the same line verbatim.",
      "presencePenalty": "How much to penalize new tokens based on whether they appear in the text so far. Increases the model's likelihood to talk about new topics.",
      "maxThinkingTokens": "The Maximum Thinking Tokens parameter determines the maximum number of tokens Claude is allowed use for its internal reasoning process. Larger budgets can improve response quality by enabling more thorough analysis for complex problems."
    }
  },
  "openai": {
    "defaultTemperature": 1,
    "maxTemperature": 2,
    "minMaxTokens": 1024,
    "defaultMaxTokens": 8192,
    "defaultThinkingTokens": 4096,
    "defaultTopP": 1,
    "minTopP": 0,
    "maxTopP": 1,
    "defaultTopK": 0,
    "minTopK": 0,
    "maxTopK": 500,
    "defaultFrequencyPenalty": 0,
    "defaultPresencePenalty": 0,
    "maxFrequencyPenalty": 2,
    "maxPresencePenalty": 2,
    "maxStopSequences": 4,
    "hint": {
      "temperature": "Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.",
      "maxTokens": "The maximum number of tokens to generate as output. The exact limit will be based on the model's total capacity and varies by model. (One token is roughly 4 characters of English text)",
      "stopSequences": "Up to four sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence. Up to 4 sequences.",
      "topP": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered.",
      "topK": "",
      "frequencyPenalty": "How much to penalize new tokens based on their existing frequency in the text so far. Decreases the model's likelihood to repeat the same line verbatim.",
      "presencePenalty": "How much to penalize new tokens based on whether they appear in the text so far. Increases the model's likelihood to talk about new topics.",
      "maxThinkingTokens": ""
    }
  },
  "cohere": {
    "defaultTemperature": 0.75,
    "maxTemperature": 5,
    "minMaxTokens": 4,
    "defaultMaxTokens": 100,
    "defaultThinkingTokens": 100,
    "defaultTopP": 0.75,
    "minTopP": 0.01,
    "maxTopP": 0.99,
    "defaultTopK": 0,
    "minTopK": 0,
    "maxTopK": 500,
    "defaultFrequencyPenalty": 0,
    "defaultPresencePenalty": 0,
    "maxFrequencyPenalty": 1,
    "maxPresencePenalty": 1,
    "maxStopSequences": 10,
    "hint": {
      "temperature": "Control the randomness aspect of which tokens the model picks for output. Value of 1 is a good starting point for experimentation. Lower values are used in tasks with a “correct” answer (e.g. question answering or summarization). Higher values enable the model to generate more “creative” outputs.",
      "maxTokens": "The maximum number of tokens to generate as output. The exact limit will be based on the model's total capacity and varies by model. (One token is roughly 4 characters of English text)",
      "stopSequences": "A stop sequence will cut off your generation at the end of the sequence. For line breaks use '\\n'. Up to 10 sequences.",
      "topP": "Ensures that only the most likely tokens, with total probability mass of P, are considered for generation at each step. If both Top K and Top P are enabled, Top P acts after Top K.",
      "topK": "When picking output tokens, consider only this number of tokens which have the highest output probability scores. 0 means top-k is not used for picking output tokens.",
      "frequencyPenalty": "Used to reduce repetitiveness of generated tokens. The higher the value, the stronger a penalty is applied to previously present tokens, proportional to how many times they have already appeared in the prompt or prior generation.",
      "presencePenalty": "Can be used to reduce repetitiveness of generated tokens. Similar to Frequency Penalty, except that this penalty is applied equally to all tokens that have already appeared, regardless of their exact frequencies.",
      "maxThinkingTokens": ""
    }
  },
  "togetherai": {
    "defaultTemperature": 0.7,
    "maxTemperature": 2,
    "minMaxTokens": 4,
    "defaultMaxTokens": 8192,
    "defaultThinkingTokens": 4096,
    "defaultTopP": 0.7,
    "minTopP": 0,
    "maxTopP": 1,
    "defaultTopK": 50,
    "minTopK": 1,
    "maxTopK": 100,
    "defaultFrequencyPenalty": 0,
    "defaultPresencePenalty": 0,
    "maxFrequencyPenalty": 2,
    "maxPresencePenalty": 2,
    "maxStopSequences": 1,
    "hint": {
      "temperature": "Creativity and randomness of the response",
      "maxTokens": "The maximum number of tokens to generate as output. The exact limit will be based on the model's total capacity and varies by model. (One token is roughly 4 characters of English text)",
      "stopSequences": "To control output length in complex queries, use stop words. Set a stop word (e.g., '\\n\\n') to signal the model to conclude the sentence, ensuring concise and relevant responses. Up to 1 sequence.",
      "topP": "Dynamically adjusts the number of choices for each predicted token, which helps to maintain diversity and generate more fluent and natural-sounding text",
      "topK": "Limits the number of choices for the next predicted word or token, which helps to speed up the generation process and can improve the quality of the generated text",
      "frequencyPenalty": "Controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition.",
      "presencePenalty": "",
      "maxThinkingTokens": ""
    }
  },
  "anthropic": {
    "defaultTemperature": 1,
    "maxTemperature": 1,
    "minMaxTokens": 2048,
    "defaultMaxTokens": 8192,
    "defaultThinkingTokens": 4096,
    "defaultTopP": 0,
    "minTopP": 0,
    "maxTopP": 1,
    "defaultTopK": 0,
    "minTopK": 0,
    "maxTopK": 100,
    "defaultFrequencyPenalty": 0,
    "defaultPresencePenalty": 0,
    "maxFrequencyPenalty": 2,
    "maxPresencePenalty": 2,
    "maxStopSequences": 4,
    "hint": {
      "temperature": "Tunes the degree of randomness in generation. Lower temperatures mean less random generations.",
      "maxTokens": "The maximum number of tokens to generate as output. The exact limit will be based on the model's total capacity and varies by model. (One token is roughly 4 characters of English text)",
      "stopSequences": "Custom text sequences that will cause the model to stop generating. Up to 4 sequences.",
      "topP": "Try nucleus sampling for diverse outputs. Set Top P to control probability cutoff for token selection. You should adjust either temperature or Top P, not both.",
      "topK": "Sample from top K options for cleaner outputs. Removes low-probability 'long tail' responses.",
      "frequencyPenalty": "",
      "presencePenalty": "",
      "maxThinkingTokens": "The 'Max Thinking Tokens' parameter determines the maximum number of tokens Claude is allowed to use for its internal reasoning process. Larger budgets can improve response quality by enabling more thorough analysis for complex problems. 'Max Thinking Tokens' must be set to a value less than 'Maximum Output Tokens'."
    }
  },
  "googleai": {
    "defaultTemperature": 0.9,
    "maxTemperature": 1,
    "minMaxTokens": 4,
    "defaultMaxTokens": 8192,
    "defaultThinkingTokens": 4096,
    "defaultTopP": 1,
    "minTopP": 0,
    "maxTopP": 1,
    "defaultTopK": 1,
    "minTopK": 0,
    "maxTopK": 40,
    "defaultFrequencyPenalty": 0,
    "defaultPresencePenalty": 0,
    "maxFrequencyPenalty": 2,
    "maxPresencePenalty": 2,
    "maxStopSequences": 10,
    "hint": {
      "temperature": "Tunes the degree of randomness in generation. Lower temperatures mean less random generations.",
      "maxTokens": "The maximum number of tokens to generate as output. The exact limit will be based on the model's total capacity and varies by model. (One token is roughly 4 characters of English text)",
      "stopSequences": "Custom text sequences that will cause the model to stop generating. Up to 10 sequences.",
      "topP": "Try nucleus sampling for diverse outputs. Set Top P to control probability cutoff for token selection. You should adjust either temperature or Top P, not both.",
      "topK": "Sample from top K options for cleaner outputs. Removes low-probability 'long tail' responses.",
      "frequencyPenalty": "",
      "presencePenalty": "",
      "maxThinkingTokens": ""
    }
  },
  "groq": {
    "defaultTemperature": 0.2,
    "maxTemperature": 1,
    "minMaxTokens": 4,
    "defaultMaxTokens": 8192,
    "defaultThinkingTokens": 4096,
    "defaultTopP": 0.8,
    "minTopP": 0,
    "maxTopP": 1,
    "defaultTopK": 1,
    "minTopK": 0,
    "maxTopK": 40,
    "defaultFrequencyPenalty": 0,
    "defaultPresencePenalty": 0,
    "maxFrequencyPenalty": 2,
    "maxPresencePenalty": 2,
    "maxStopSequences": 10,
    "hint": {
      "temperature": "Tunes the degree of randomness in generation. Lower temperatures mean less random generations.",
      "maxTokens": "The maximum number of tokens to generate as output. The exact limit will be based on the model's total capacity and varies by model. (One token is roughly 4 characters of English text)",
      "stopSequences": "Custom text sequences that will cause the model to stop generating. Up to 10 sequences.",
      "topP": "Try nucleus sampling for diverse outputs. Set Top P to control probability cutoff for token selection. You should adjust either temperature or Top P, not both.",
      "topK": "Sample from top K options for cleaner outputs. Removes low-probability 'long tail' responses.",
      "frequencyPenalty": "",
      "presencePenalty": "",
      "maxThinkingTokens": ""
    }
  },
  "bedrock": {
    "defaultTemperature": 1,
    "maxTemperature": 1,
    "minMaxTokens": 4,
    "defaultMaxTokens": 8192,
    "defaultThinkingTokens": 4096,
    "defaultTopP": 0.8,
    "minTopP": 0,
    "maxTopP": 1,
    "defaultTopK": 1,
    "minTopK": 0,
    "maxTopK": 500,
    "defaultFrequencyPenalty": 0,
    "defaultPresencePenalty": 0,
    "maxFrequencyPenalty": 2,
    "maxPresencePenalty": 2,
    "maxStopSequences": 4,
    "hint": {
      "temperature": "Tunes the degree of randomness in generation. Lower temperatures mean less random generations.",
      "maxTokens": "The maximum number of tokens to generate as output (one token is roughly 4 characters of English text). Note that not all foundation models support the maximum of 8192 tokens. Please refer to the specific model documentation for the exact limit.",
      "stopSequences": "Custom text sequences that will cause the model to stop generating. Up to 4 sequences.",
      "topP": "Try nucleus sampling for diverse outputs. Set Top P to control probability cutoff for token selection. You should adjust either temperature or Top P, not both.",
      "topK": "",
      "frequencyPenalty": "",
      "presencePenalty": "",
      "maxThinkingTokens": ""
    }
  },
  "vertexai": {
    "defaultTemperature": 0.9,
    "maxTemperature": 1,
    "minMaxTokens": 4,
    "defaultMaxTokens": 8192,
    "defaultThinkingTokens": 4096,
    "defaultTopP": 1,
    "minTopP": 0,
    "maxTopP": 1,
    "defaultTopK": 1,
    "minTopK": 1,
    "maxTopK": 40,
    "defaultFrequencyPenalty": 0,
    "defaultPresencePenalty": 0,
    "maxFrequencyPenalty": 2,
    "maxPresencePenalty": 2,
    "maxStopSequences": 10,
    "hint": {
      "temperature": "Tunes the degree of randomness in generation. Lower temperatures mean less random generations.",
      "maxTokens": "The maximum number of tokens to generate as output (one token is roughly 4 characters of English text). Note that not all foundation models support the maximum of 8192 tokens. Please refer to the specific model documentation for the exact limit.",
      "stopSequences": "Custom text sequences that will cause the model to stop generating. Up to 10 sequences.",
      "topP": "Try nucleus sampling for diverse outputs. Set Top P to control probability cutoff for token selection. You should adjust either temperature or Top P, not both.",
      "topK": "Sample from top K options for cleaner outputs. Removes low-probability 'long tail' responses.",
      "frequencyPenalty": "",
      "presencePenalty": "",
      "maxThinkingTokens": ""
    }
  },
  "perplexity": {
    "defaultTemperature": 0.2,
    "maxTemperature": 2,
    "minMaxTokens": 4,
    "defaultMaxTokens": 8192,
    "defaultThinkingTokens": 4096,
    "defaultTopP": 0.9,
    "minTopP": 0,
    "maxTopP": 1,
    "defaultTopK": 0,
    "minTopK": 0,
    "maxTopK": 2048,
    "defaultFrequencyPenalty": 1,
    "defaultPresencePenalty": 0,
    "maxFrequencyPenalty": 2,
    "maxPresencePenalty": 2,
    "maxStopSequences": 4,
    "hint": {
      "temperature": "The amount of randomness in the response, valued between 0 inclusive and 2 exclusive. Higher values are more random, and lower values are more deterministic.",
      "maxTokens": "The maximum number of completion tokens returned by the API. The number of tokens requested in max_tokens plus the number of prompt tokens sent in messages must not exceed the context window token limit of model requested. If left unspecified, then the model will generate tokens until either it reaches its stop token or the end of its context window.",
      "stopSequences": "",
      "topP": "The nucleus sampling threshold, valued between 0 and 1 inclusive. For each subsequent token, the model considers the results of the tokens with top_p probability mass. We recommend either altering top_k or top_p, but not both.",
      "topK": "The number of tokens to keep for highest top-k filtering, specified as an integer between 0 and 2048 inclusive. If set to 0, top-k filtering is disabled. We recommend either altering top_k or top_p, but not both.",
      "frequencyPenalty": "A value between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Incompatible with frequency_penalty.",
      "presencePenalty": "A multiplicative penalty greater than 0. Values greater than 1.0 penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. A value of 1.0 means no penalty. Incompatible with presence_penalty.",
      "maxThinkingTokens": ""
    }
  },
  "xai": {
    "defaultTemperature": 0.2,
    "maxTemperature": 2,
    "minMaxTokens": 4,
    "defaultMaxTokens": 8192,
    "defaultThinkingTokens": 4096,
    "defaultTopP": 0.9,
    "minTopP": 0,
    "maxTopP": 1,
    "defaultTopK": 0,
    "minTopK": 0,
    "maxTopK": 2048,
    "defaultFrequencyPenalty": 0,
    "defaultPresencePenalty": 0,
    "maxFrequencyPenalty": 2,
    "maxPresencePenalty": 2,
    "maxStopSequences": 0,
    "hint": {
      "temperature": "Controls randomness in the response. Higher values make the output more creative and random, while lower values make it more focused and deterministic.",
      "maxTokens": "The maximum number of tokens to generate as output. The exact limit will be based on the model's total capacity and varies by model. (One token is roughly 4 characters of English text)",
      "stopSequences": "",
      "topP": "Controls diversity via nucleus sampling. Lower values make the output more focused, while higher values make it more diverse.",
      "topK": "Limits the number of token choices for each step. Setting to 0 disables top-k filtering.",
      "frequencyPenalty": "",
      "presencePenalty": "",
      "maxThinkingTokens": ""
    }
  },
  "ollama": {
    "defaultTemperature": 0.8,
    "maxTemperature": 2,
    "minMaxTokens": 1,
    "defaultMaxTokens": 128,
    "defaultThinkingTokens": 128,
    "defaultTopP": 0.9,
    "minTopP": 0,
    "maxTopP": 1,
    "defaultTopK": 40,
    "minTopK": 1,
    "maxTopK": 100,
    "defaultFrequencyPenalty": 1.1,
    "defaultPresencePenalty": 0,
    "maxFrequencyPenalty": 2,
    "maxPresencePenalty": 2,
    "maxStopSequences": 4,
    "hint": {
      "temperature": "Controls randomness in the response. Higher values make the output more creative and random, while lower values make it more focused and deterministic. Default: 0.8",
      "maxTokens": "The maximum number of tokens to predict. Set to -1 for unlimited generation or -2 to fill the context. Default: 128 tokens (one token is roughly 4 characters of English text)",
      "stopSequences": "Custom text sequences that will cause the model to stop generating when encountered.",
      "topP": "Controls diversity via nucleus sampling (0-1). Lower values make the output more focused, while higher values make it more diverse. Default: 0.9",
      "topK": "Limits the number of tokens considered at each step to reduce nonsensical outputs. Higher values allow more diversity. Default: 40",
      "frequencyPenalty": "Repeat penalty strength to prevent repetition. Values greater than 1.0 penalize repeated tokens more strongly. Default: 1.1",
      "presencePenalty": "",
      "maxThinkingTokens": ""
    }
  }
}
